{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09659222",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea007753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes available (Corel-1K): ['africans', 'beaches', 'buildings', 'buses', 'dinosaurs', 'elephants', 'flowers', 'food', 'horses', 'mountains']\n",
      "Classes available (GHIM-10K): ['aircraft', 'buildings', 'butterfly', 'cars', 'dragon fly', 'fireworks', 'flowers', 'hen', 'horses', 'insects', 'motorcycles', 'mountains', 'sea shores', 'ships', 'sunset', 'temples', 'trees', 'valleys', 'walls', 'yacht']\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import Block\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Dataset Directories\n",
    "data_dir = \"D:\\\\Trade-Off_CBIR\\\\dataset\\\\Corel-1K\"\n",
    "data_dir_GHIM = \"D:\\\\Trade-Off_CBIR\\\\dataset\\\\GHIM-10K\"\n",
    "\n",
    "print(\"Classes available (Corel-1K):\", os.listdir(data_dir))\n",
    "print(\"Classes available (GHIM-10K):\", os.listdir(data_dir_GHIM))\n",
    "\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# Image Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b17da9a",
   "metadata": {},
   "source": [
    "## Dataset: Corel-1K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b6069e",
   "metadata": {},
   "source": [
    "### Tahap 1: Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "196ba653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "\n",
    "class HOGBackbone:\n",
    "    def __init__(self, resize_shape=(128, 128)):\n",
    "        self.resize_shape = resize_shape\n",
    "        self.hog = cv2.HOGDescriptor(\n",
    "            _winSize=resize_shape,\n",
    "            _blockSize=(16,16),\n",
    "            _blockStride=(8,8),\n",
    "            _cellSize=(8,8),\n",
    "            _nbins=9\n",
    "        )\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(img, self.resize_shape)\n",
    "        feat = self.hog.compute(img)\n",
    "        return feat.flatten()\n",
    "\n",
    "\n",
    "class GISTBackbone:\n",
    "    def __init__(self, resize_shape=(128,128), num_blocks=4):\n",
    "        self.resize_shape = resize_shape\n",
    "        self.num_blocks = num_blocks\n",
    "        self.thetas = [0, np.pi/6, np.pi/4, np.pi/3, np.pi/2]\n",
    "        self.scales = [4, 8, 16]\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(img, self.resize_shape)\n",
    "\n",
    "        h, w = self.resize_shape\n",
    "        bh, bw = h // self.num_blocks, w // self.num_blocks\n",
    "        feats = []\n",
    "\n",
    "        for i in range(self.num_blocks):\n",
    "            for j in range(self.num_blocks):\n",
    "                patch = img[i*bh:(i+1)*bh, j*bw:(j+1)*bw]\n",
    "                for theta in self.thetas:\n",
    "                    for k in self.scales:\n",
    "                        kernel = cv2.getGaborKernel(\n",
    "                            (k,k), 4.0, theta, 10.0, 0.5, 0\n",
    "                        )\n",
    "                        f = cv2.filter2D(patch, cv2.CV_32F, kernel)\n",
    "                        feats.extend([f.mean(), f.std()])\n",
    "\n",
    "        return np.array(feats, dtype=np.float32)\n",
    "\n",
    "\n",
    "class GLCMBackbone:\n",
    "    def __init__(\n",
    "        self,\n",
    "        resize_shape=(128,128),\n",
    "        distances=(1, 2),\n",
    "        angles=(0, np.pi/4, np.pi/2, 3*np.pi/4),\n",
    "        levels=256,\n",
    "        props=(\"contrast\", \"dissimilarity\", \"homogeneity\", \"energy\", \"correlation\", \"ASM\")\n",
    "    ):\n",
    "        self.resize_shape = resize_shape\n",
    "        self.distances = distances\n",
    "        self.angles = angles\n",
    "        self.levels = levels\n",
    "        self.props = props\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(img, self.resize_shape)\n",
    "\n",
    "        # IMPORTANT: GLCM expects integer levels\n",
    "        img = np.clip(img, 0, self.levels - 1).astype(np.uint8)\n",
    "\n",
    "        glcm = graycomatrix(\n",
    "            img,\n",
    "            distances=self.distances,\n",
    "            angles=self.angles,\n",
    "            levels=self.levels,\n",
    "            symmetric=True,\n",
    "            normed=True\n",
    "        )\n",
    "\n",
    "        feats = []\n",
    "        for prop in self.props:\n",
    "            vals = graycoprops(glcm, prop)\n",
    "            feats.extend(vals.flatten())\n",
    "\n",
    "        return np.array(feats, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0194cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalContextEncoder(nn.Module):\n",
    "    def __init__(self, backbone, backbone_output_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.proj = nn.Linear(backbone_output_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: batch of images (list of numpy images)\n",
    "        feats = [self.backbone(img) for img in x]\n",
    "        feats = torch.tensor(np.stack(feats), dtype=torch.float32)\n",
    "        z = self.proj(feats)\n",
    "        return z\n",
    "\n",
    "\n",
    "class ClassicalTargetEncoder(nn.Module):\n",
    "    def __init__(self, backbone, backbone_output_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.proj = nn.Linear(backbone_output_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = [self.backbone(img) for img in x]\n",
    "        feats = torch.tensor(np.stack(feats), dtype=torch.float32)\n",
    "        z = self.proj(feats)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "\n",
    "# BACKBONES\n",
    "hog_backbone = HOGBackbone()\n",
    "glcm_backbone = GLCMBackbone()\n",
    "gist_backbone = GISTBackbone()\n",
    "\n",
    "# OUTPUT DIM (FIX, STATIC)\n",
    "HOG_DIM  = len(hog_backbone(np.zeros((128,128,3), dtype=np.uint8)))\n",
    "GLCM_DIM = len(glcm_backbone(np.zeros((128,128,3), dtype=np.uint8)))\n",
    "GIST_DIM = len(gist_backbone(np.zeros((128,128,3), dtype=np.uint8)))\n",
    "\n",
    "# CONTEXT ENCODERS (CLASSICAL)\n",
    "context_encoders = {\n",
    "    \"hog\":  ClassicalContextEncoder(hog_backbone,  HOG_DIM,  256),\n",
    "    \"glcm\": ClassicalContextEncoder(glcm_backbone, GLCM_DIM, 256),\n",
    "    \"gist\": ClassicalContextEncoder(gist_backbone, GIST_DIM, 256)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6326ad74",
   "metadata": {},
   "source": [
    "### Tahap 2: Feature Extraction (JOINT-EMBEDDING MODELS INITIALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43e6b70d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ClassicalContextEncoder.__init__() missing 1 required positional argument: 'embedding_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m GLCM_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(glcm(dummy))\n\u001b[0;32m      9\u001b[0m GIST_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(gist(dummy))\n\u001b[0;32m     11\u001b[0m context_encoders \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhog\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[43mClassicalContextEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mHOG_DIM\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglcm\u001b[39m\u001b[38;5;124m\"\u001b[39m: ClassicalContextEncoder(glcm, GLCM_DIM),\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgist\u001b[39m\u001b[38;5;124m\"\u001b[39m: ClassicalContextEncoder(gist, GIST_DIM)\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     17\u001b[0m models_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhog\u001b[39m\u001b[38;5;124m\"\u001b[39m:  {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m\"\u001b[39m: hog,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m: context_encoders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhog\u001b[39m\u001b[38;5;124m\"\u001b[39m]},\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglcm\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m\"\u001b[39m: glcm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m: context_encoders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglcm\u001b[39m\u001b[38;5;124m\"\u001b[39m]},\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgist\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m\"\u001b[39m: gist, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m: context_encoders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgist\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m     21\u001b[0m }\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# FEATURE STORAGE (SAMA PERSIS DENGAN CNN)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ClassicalContextEncoder.__init__() missing 1 required positional argument: 'embedding_dim'"
     ]
    }
   ],
   "source": [
    "hog = HOGBackbone()\n",
    "glcm = GLCMBackbone()\n",
    "gist = GISTBackbone()\n",
    "\n",
    "# output dim (static)\n",
    "dummy = np.zeros((128,128,3), dtype=np.uint8)\n",
    "HOG_DIM  = len(hog(dummy))\n",
    "GLCM_DIM = len(glcm(dummy))\n",
    "GIST_DIM = len(gist(dummy))\n",
    "\n",
    "context_encoders = {\n",
    "    \"hog\":  ClassicalContextEncoder(hog,  HOG_DIM),\n",
    "    \"glcm\": ClassicalContextEncoder(glcm, GLCM_DIM),\n",
    "    \"gist\": ClassicalContextEncoder(gist, GIST_DIM)\n",
    "}\n",
    "\n",
    "models_dict = {\n",
    "    \"hog\":  {\"baseline\": hog,  \"pretrained\": context_encoders[\"hog\"]},\n",
    "    \"glcm\": {\"baseline\": glcm, \"pretrained\": context_encoders[\"glcm\"]},\n",
    "    \"gist\": {\"baseline\": gist, \"pretrained\": context_encoders[\"gist\"]}\n",
    "}\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# FEATURE STORAGE (SAMA PERSIS DENGAN CNN)\n",
    "# =====================================================\n",
    "\n",
    "features_dict = {\n",
    "    m: {t: [] for t in [\"baseline\", \"pretrained\"]}\n",
    "    for m in models_dict\n",
    "}\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# FEATURE EXTRACTION LOOP (CNN-STYLE)\n",
    "# =====================================================\n",
    "\n",
    "for path in tqdm(img_paths, desc=\"Extracting Classical Features\"):\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    for m_name in models_dict:\n",
    "        for t_name in models_dict[m_name]:\n",
    "\n",
    "            if t_name == \"pretrained\":\n",
    "                feat = models_dict[m_name][t_name](img)\n",
    "            else:\n",
    "                feat = extract_features(models_dict[m_name][t_name], img)\n",
    "\n",
    "            features_dict[m_name][t_name].append(feat)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# L2 NORMALIZATION (SAMA PERSIS)\n",
    "# =====================================================\n",
    "\n",
    "for m_name in features_dict:\n",
    "    for t_name in features_dict[m_name]:\n",
    "        arr = np.array(features_dict[m_name][t_name])\n",
    "        features_dict[m_name][t_name] = normalize(arr).tolist()\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# EXAMPLE ACCESS (SAMA PERSIS)\n",
    "# =====================================================\n",
    "\n",
    "hog_pre   = np.array(features_dict[\"hog\"][\"pretrained\"])\n",
    "glcm_base = np.array(features_dict[\"glcm\"][\"baseline\"])\n",
    "gist_base = np.array(features_dict[\"gist\"][\"baseline\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab705a",
   "metadata": {},
   "source": [
    "### Tahap 3: Image Retrieval & Similarity Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457955a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26e9031e",
   "metadata": {},
   "source": [
    "### Tahap 4: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e536048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8988c78",
   "metadata": {},
   "source": [
    "## Dataset: GHIM-10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b169e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c644a017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d44be91",
   "metadata": {},
   "source": [
    "### Tahap 2: Feature Extraction (JOINT-EMBEDDING MODELS INITIALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6080940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a5273e8",
   "metadata": {},
   "source": [
    "### Tahap 3: Image Retrieval & Similarity Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554293d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0aef110",
   "metadata": {},
   "source": [
    "### Tahap 4: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50125d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
